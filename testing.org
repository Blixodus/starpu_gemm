* With Guix

** Guix setup

To setup Guix correctly add these to your ~$HOME/.bash_profile~
Set ~[username]~ to your username

#+begin_src shell
  . /var/guix/profiles/per-user/[username]/current-guix/etc/bash_completion.d/guix

  export GUIX_LOCPATH=$HOME/.guix-profile/lib/locale

  GUIX_PROFILE="/home/[username]/.config/guix/current"
  . "$GUIX_PROFILE/etc/profile"

  GUIX_PROFILE="/home/[username]/.guix-profile"
  . "$GUIX_PROFILE/etc/profile"
#+end_src

To setup your Guix environment you will need the correct channels (Guix-HPC and Guix-HPC-non-free). Add these to ~$HOME/.config/guix/channels.scm~

#+begin_src scheme
  (list (channel
         (name 'guix)
         (url "https://git.savannah.gnu.org/git/guix.git")
         (branch "master")
         (commit
          "f572de77c4fda3347bea16b4c86d630b193291dc"))
        (channel
         (name 'guix-hpc-non-free)
         (url "https://gitlab.inria.fr/guix-hpc/guix-hpc-non-free.git")
         (branch "master")
         (commit
          "e77426506e0384cf847db94541314896c617cad6"))
        (channel
         (name 'guix-hpc)
         (url "https://gitlab.inria.fr/guix-hpc/guix-hpc.git")
         (branch "master")
         (commit
          "81b2648583d3b6aff4d3abc999a3c7fe70907e25")))
#+end_src

Do not forget to ~guix pull~

** Environment variables to set
#+begin_src shell
  export OMP_NUM_THREADS=1     #Set OpenMP thread number to 1 to avoid conflicts with the StarPU scheduler
  export STARPU_RESERVE_NCPU=1 #Reserve 1 CPU to be used by StarPU for other things than task execution
  export STARPU_SCHED=lws      #Set the StarPU scheduler to lws (or other)

  export OMPI_MCA_btl=^openib  #Set these to avoid some MPI errors (optional - the errors are not real)
  export OMPI_MCA_osc=^ucx
  export OMPI_MCA_pml=^ucx
#+end_src

** Execute code (PlaFRIM : Works only without CUDA)
You can create a Guix shell with the GEMM example and Chameleon using the following command

#+BEGIN_SRC shell
  guix shell --pure --preserve="^OMPI_MCA|^SLURM|^STARPU|^OMP" starpu-example-cppgemm chameleon openssh slurm@22 --with-branch=starpu-example-cppgemm=main --with-branch=starpu=master
#+END_SRC

#+BEGIN_SRC shell
  LD_PRELOAD=/usr/lib64/libcuda.so guix shell --pure --preserve=^LD_PRELOAD chameleon-cuda starpu-example-cppgemm-cuda openssh --with-commit=starpu-cuda=8654bf062cddb87e7b23ef62ad13a0f5f1e18c77 --with-branch=starpu-example-cppgemm-cuda=main
#+END_SRC

In the Guix shell you can execute with e.g.

#+BEGIN_SRC shell
  gemm [params]
  chameleon_stesting [params]
  srun -l gemm [params]
  srun -l chameleon_stesting [params]
#+END_SRC

* Without Guix

** Modules
#+BEGIN_SRC shell
module load compiler/cuda/11.6
module load compiler/gcc/11.2.0
module load linalg/openblas/0.3.9
module load hardware/hwloc/2.7.0
module load trace/fxt/0.3.14
module load mpi/openmpi/4.0.3
module load build/cmake/3.21.3
module load linalg/lapacke/3.9.1
module load tools/trace/likwid/4.0.3
#+END_SRC

** Install New Madeleine (stable commit)
#+BEGIN_SRC shell
git clone https://gitlab.inria.fr/pm2/pm2.git
cd pm2
git checkout 7e3edd5002885d4a114c5e551106f28ae9630e01
cd scripts
./pm2-build-packages ./madmpi.conf --prefix=$HOME/local
#+END_SRC

** Install StarPU (stable fork)
#+begin_src
git clone https://github.com/Blixodus/starpu
./autogen.sh
mkdir buildq
cd build
# Without nmad (works on Rubik/PlaFRIM)
../configure --prefix=$HOME/local
# With nmad (works only on Rubik)'
../configure --prefix=$HOME/local --enable-nmad
make -j
make install -j 
#+end_src

** Important to set this environment
#+begin_src
source $HOME/local/bin/starpu_env
export OMP_NUM_THREADS=1 
#+end_src

** Install chameleon
#+begin_src
git clone --recurse-submodules https://gitlab.inria.fr/solverstack/chameleon.git
cd chameleon
cmake -B build -DCHAMELEON_USE_CUDA=ON -DCHAMELEON_USE_MPI=ON -DBUILD_SHARED_LIBS=ON -DCMAKE_INSTALL_PREFIX=$HOME/local
cd build
make -j
make install -j 
#+end_src

** Build starpu_gemm
#+begin_src
git clone git@github.com:Blixodus/starpu_gemm.git
cd starpu_gemm
cmake -B build -DENABLE_CUDA=ON
cd build
make -j 
#+end_src

** MPI Check
#+begin_src
mpirun -n 2 --map-by node --tag-output hostname 
#+end_src

** Run with MPI
#+begin_src
mpirun -n 2 --map-by node --tag-output gemm [exp] [k_min] [k_max] [bs_min] [bs_max]
mpirun -n 2 --map-by node --tag-output chameleon_stesting -H -o gemm -m 20000 -n 20000 -k 2000 -b 2000 -g 2 
#+end_src

** Run with nmad
#+begin_src
padico-launch -n 2 
#+end_src
